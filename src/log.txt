tokens per iteration will be: 524,288
Initializing a new model from scratch
defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)
number of parameters: 123.63M
num decayed parameter tensors: 50, with 124,354,560 parameters
num non-decayed parameter tensors: 49, with 65,280 parameters
using fused AdamW: True
